# app.py
import os
import time
import json
import logging
import torch
from flask import Flask, render_template, request, jsonify, session
# å…¼å®¹werkzeugç‰ˆæœ¬é—®é¢˜
try:
    from werkzeug.urls import url_quote
except ImportError:
    from werkzeug.utils import url_quote  # æ–°ç‰ˆæœ¬ä¸­çš„ä½ç½®
except ImportError:
    from urllib.parse import quote as url_quote  # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
from Core.models import DocumentQASystem
from Core.document_loader import load_document
from Core.auto_test import perform_auto_test, visualize_results
from Utils.utils import export_to_excel, generate_improved_prompt, save_feedback_data

app = Flask(__name__)
app.secret_key = os.urandom(24)  # ç”¨äºsessionåŠ å¯†

# å…¨å±€QAç³»ç»Ÿå®ä¾‹
qa_system = DocumentQASystem()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# è·¯ç”±ï¼šä¸»é¡µ
@app.route('/')
def index():
    return render_template('index.html', models=list(qa_system.llm_registry.keys()))

# è·¯ç”±ï¼šå¤„ç†é—®ç­”è¯·æ±‚
@app.route('/query', methods=['POST'])
def query():
    data = request.json
    query_text = data.get('query', '')
    model_name = data.get('model', qa_system.current_model)
    
    # ç¡®ä¿é€‰æ‹©çš„æ¨¡å‹æœ‰æ•ˆ
    if model_name not in qa_system.llm_registry:
        return jsonify({'error': f'æ— æ•ˆæ¨¡å‹ï¼Œå¯ç”¨é€‰é¡¹ï¼š{list(qa_system.llm_registry.keys())}'}), 400
    
    # å¦‚æœæ¨¡å‹ä¸æ˜¯å½“å‰æ¨¡å‹ï¼Œåˆ™åˆ‡æ¢
    if model_name != qa_system.current_model:
        qa_system.current_model = model_name
        qa_system._release_model_resources()
        qa_system.conversation_chain = None
    
    try:
        start_time = time.time()
        
        # å¤„ç†æŸ¥è¯¢
        if qa_system.qa_chain:  # æ–‡æ¡£é—®ç­”æ¨¡å¼
            result = qa_system.qa_chain({"query": query_text})
            response = f"{result['result']}\næ¥æºï¼š{result['source_documents'][0].metadata['source']}"
        else:  # æ™®é€šå¯¹è¯æ¨¡å¼
            if not qa_system.conversation_chain:
                from langchain.chains import ConversationChain
                qa_system.conversation_chain = ConversationChain(
                    llm=qa_system.llm_registry[qa_system.current_model],
                    memory=qa_system.memory
                )
            response = qa_system.conversation_chain.predict(input=query_text)
        
        end_time = time.time()
        latency = end_time - start_time
        
        # ä¿å­˜åˆ°sessionä»¥ä¾¿åé¦ˆ
        session['last_query'] = query_text
        session['last_response'] = response
        session['last_model'] = model_name
        
        return jsonify({
            'response': response,
            'model': model_name.upper(),
            'latency': f"{latency:.2f}s"
        })
    
    except Exception as e:
        logging.error(f"å¤„ç†é”™è¯¯ï¼š{str(e)}")
        return jsonify({'error': f"å¤„ç†é”™è¯¯ï¼š{str(e)}"}), 500

# è·¯ç”±ï¼šå¤„ç†æ–‡ä»¶ä¸Šä¼ 
@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    upload_folder = os.path.join(os.getcwd(), 'uploads')
    os.makedirs(upload_folder, exist_ok=True)
    file_path = os.path.join(upload_folder, file.filename)
    file.save(file_path)
    
    # åŠ è½½æ–‡æ¡£
    if load_document(qa_system, file_path):
        return jsonify({'success': True, 'message': 'æ–‡æ¡£åŠ è½½æˆåŠŸ'})
    else:
        return jsonify({'error': 'æ–‡æ¡£åŠ è½½å¤±è´¥'}), 500

# è·¯ç”±ï¼šå¤„ç†åé¦ˆ
@app.route('/feedback', methods=['POST'])
def feedback():
    data = request.json
    feedback_type = data.get('type')  # 'like' æˆ– 'dislike'
    reason = data.get('reason', '')  # ä¸æ»¡æ„çš„åŸå› 
    
    # è·å–ä¸Šæ¬¡çš„æŸ¥è¯¢å’Œå“åº”
    last_query = session.get('last_query')
    last_response = session.get('last_response')
    last_model = session.get('last_model')
    
    if not last_query or not last_response:
        return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ä»¥è¯„ä»·çš„ä¸Šä¸€æ¬¡å¯¹è¯'}), 400
    
    if feedback_type == 'like':
        # ä¿å­˜æ­£é¢åé¦ˆ
        save_feedback_data(last_query, last_response, last_model, "like")
        return jsonify({'success': True, 'message': 'æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼æˆ‘ä»¬ä¼šç»§ç»­ä¿æŒè¿™æ ·çš„å›ç­”è´¨é‡ã€‚'})
    
    elif feedback_type == 'dislike':
        # ç”Ÿæˆæ”¹è¿›çš„æç¤ºè¯
        improved_prompt = generate_improved_prompt(last_query, last_response, "dislike", reason)
        
        try:
            # ä½¿ç”¨æ”¹è¿›çš„æç¤ºè¯é‡æ–°ç”Ÿæˆå›ç­”
            model = qa_system.llm_registry[qa_system.current_model]
            combined_prompt = f"{improved_prompt}\n\nåŸå§‹é—®é¢˜: {last_query}"
            improved_response = model.invoke(combined_prompt)
            
            # ä¿å­˜åé¦ˆå’Œæ”¹è¿›çš„å›ç­”
            save_feedback_data(last_query, last_response, last_model, "dislike", improved_response, reason)
            
            # æ›´æ–°sessionä¸­çš„æœ€è¿‘å“åº”
            session['last_response'] = improved_response
            
            return jsonify({
                'success': True, 
                'message': 'æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼', 
                'improved_response': improved_response
            })
            
        except Exception as e:
            logging.error(f"ç”Ÿæˆæ”¹è¿›å›ç­”æ—¶å‡ºé”™ï¼š{str(e)}")
            return jsonify({'error': f"ç”Ÿæˆæ”¹è¿›å›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"}), 500
    
    return jsonify({'error': 'æ— æ•ˆçš„åé¦ˆç±»å‹'}), 400

# è·¯ç”±ï¼šæ¨¡å‹æ¯”è¾ƒ
@app.route('/compare', methods=['POST'])
def compare():
    data = request.json
    query_text = data.get('query', '')
    
    if not query_text:
        return jsonify({'error': 'è¯·æä¾›æŸ¥è¯¢å†…å®¹'}), 400
    
    results = {}
    for name, model in qa_system.llm_registry.items():
        try:
            # å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ç­‰å¾…ç³»ç»Ÿç¨³å®š
            time.sleep(1)
            
            # è®°å½•å¼€å§‹æ—¶çš„GPUæ˜¾å­˜ä½¿ç”¨
            gpu_memory_before = qa_system.get_gpu_memory_usage()
            
            start_time = time.time()
            if qa_system.qa_chain:  # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£ä¸Šä¼ 
                # ä½¿ç”¨qa_chainç”ŸæˆåŸºäºæ–‡æ¡£çš„å›ç­”
                from langchain.chains import RetrievalQA
                qa_system.qa_chain = RetrievalQA.from_chain_type(
                    llm=model,
                    chain_type="stuff",
                    retriever=qa_system.vector_db.as_retriever(search_kwargs={"k": 3}),
                    return_source_documents=True
                )
                result = qa_system.qa_chain({"query": query_text})
                response = f"{result['result']}\nğŸ“š æ¥æºï¼š{result['source_documents'][0].metadata['source']}"
            else:
                # æ²¡æœ‰æ–‡æ¡£ä¸Šä¼ ï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹
                response = model.invoke(query_text)
            end_time = time.time()
            
            # ç­‰å¾…ç³»ç»Ÿç¨³å®šåå†æµ‹é‡æ˜¾å­˜
            time.sleep(1)
            
            # å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # è®°å½•ç»“æŸæ—¶çš„GPUæ˜¾å­˜ä½¿ç”¨
            gpu_memory_after = qa_system.get_gpu_memory_usage()
            
            # è®¡ç®—GPUæ˜¾å­˜å˜åŒ–
            gpu_memory_diff = {}
            if gpu_memory_before["available"] and gpu_memory_after["available"]:
                for device_id, before_stats in gpu_memory_before["devices"].items():
                    after_stats = gpu_memory_after["devices"][device_id]
                    memory_diff = round(after_stats["used_memory_mb"] - before_stats["used_memory_mb"], 2)
                    peak_memory = after_stats.get("peak_memory_mb", after_stats["used_memory_mb"])
                    
                    gpu_memory_diff[device_id] = {
                        "device_name": before_stats["device_name"],
                        "memory_diff_mb": memory_diff,
                        "peak_memory_mb": peak_memory
                    }
            
            latency = end_time - start_time
            
            # ä¼°ç®—ä»¤ç‰Œæ•°é‡ (ç®€å•ä¼°è®¡ï¼Œæ¯ä¸ªå•è¯çº¦1.3ä¸ªä»¤ç‰Œ)
            response_words = len(response.split())
            estimated_tokens = int(response_words * 1.3)
            
            # è®¡ç®—ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦
            tokens_per_second = estimated_tokens / latency if latency > 0 else 0
            
            results[name] = {
                "response": response,
                "latency": f"{latency:.2f}s",
                "tokens": estimated_tokens,
                "tokens_per_second": f"{tokens_per_second:.2f}",
                "response_length": len(response),
                "gpu_memory_diff": gpu_memory_diff
            }
            
            # é‡Šæ”¾èµ„æº
            del response
            qa_system._release_model_resources()
        except Exception as e:
            results[name] = {"error": str(e)}
    
    # å¯¼å‡ºç»“æœåˆ°Excel
    export_file = export_to_excel(results, query_text)
    export_path = os.path.abspath(export_file) if export_file else None
    
    return jsonify({
        'results': results,
        'export_file': export_path
    })

# è·¯ç”±ï¼šè‡ªåŠ¨æµ‹è¯•
@app.route('/autotest', methods=['POST'])
def autotest():
    data = request.json
    logic_count = int(data.get('logic_count', 0))
    read_count = int(data.get('read_count', 0))
    math_count = int(data.get('math_count', 0))
    
    if logic_count <= 0 and read_count <= 0 and math_count <= 0:
        return jsonify({'error': 'è¯·è‡³å°‘é€‰æ‹©ä¸€ç§é¢˜å‹è¿›è¡Œæµ‹è¯•'}), 400
    
    try:
        # æ‰§è¡Œè‡ªåŠ¨æµ‹è¯•
        results, standard_answers = perform_auto_test(qa_system, logic_count, read_count, math_count)
        
        # å¯¼å‡ºç»“æœ
        from Core.auto_test import export_to_excel as export_test_results
        export_file = export_test_results(results, standard_answers)
        
        if export_file:
            # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            chart_file = visualize_results(export_file)
            return jsonify({
                'success': True,
                'message': 'æµ‹è¯•å®Œæˆ',
                'export_file': os.path.abspath(export_file),
                'chart_file': os.path.abspath(chart_file) if chart_file else None
            })
        else:
            return jsonify({'error': 'å¯¼å‡ºç»“æœå¤±è´¥'}), 500
    
    except Exception as e:
        logging.error(f"è‡ªåŠ¨æµ‹è¯•é”™è¯¯ï¼š{str(e)}")
        return jsonify({'error': f"è‡ªåŠ¨æµ‹è¯•é”™è¯¯ï¼š{str(e)}"}), 500

# å¯åŠ¨åº”ç”¨
if __name__ == '__main__':
    # ç¡®ä¿ä¸Šä¼ å’Œåé¦ˆç›®å½•å­˜åœ¨
    os.makedirs("./uploads", exist_ok=True)
    os.makedirs("./feedback_data", exist_ok=True)
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(debug=True, host='0.0.0.0', port=5000)