# document_loader.py
import os
import logging
import torch
import chardet
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

device = "cuda" if torch.cuda.is_available() else "cpu"
def detect_encoding(file_path, max_size=1024*1024):
    """å®‰å…¨æ£€æµ‹æ–‡æœ¬æ–‡ä»¶ç¼–ç """
    try:
        with open(file_path, 'rb') as f:
            raw_data = b''
            chunk_size = 4096
            while len(raw_data) < max_size:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                raw_data += chunk
            return chardet.detect(raw_data)['encoding']
    except Exception as e:
        logging.error(f"ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}")
        return 'utf-8'

def load_document(qa_system, file_path):
    """æ–‡æ¡£åŠ è½½å¤„ç†"""
    print(f"ğŸ“„  è¯»å–æ–‡ä»¶ï¼š{file_path}")
    try:
        if not os.path.isfile(file_path):
            raise ValueError("è·¯å¾„ä¸æ˜¯æ–‡ä»¶")

        # ç¼–ç æ£€æµ‹å’Œå¤‡é€‰ç¼–ç åˆ—è¡¨
        encoding = detect_encoding(file_path)
        retry_encodings = [encoding, 'utf-8', 'gbk']

        loader = None
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.lower().endswith(('.docx', '.doc')):
            loader = Docx2txtLoader(file_path)
        else:
            for enc in retry_encodings:
                try:
                    loader = TextLoader(file_path, encoding=enc)
                    loader.load()  # æµ‹è¯•åŠ è½½
                    break
                except UnicodeDecodeError:
                    continue

        documents = loader.load()

        # ä½¿ç”¨é€’å½’æ–‡æœ¬åˆ†å‰²å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦â€¦", "â€¦", "ã€€"]
        )
        docs = text_splitter.split_documents(documents)

        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æœ¬å—")
        if docs:
            print(f"ğŸ“ é¦–æ–‡æœ¬å—ç¤ºä¾‹ï¼š{docs[0].page_content[:200]}...")

        # æ„é€ å‘é‡æ•°æ®åº“
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={"device": device},
            encode_kwargs={"batch_size": 32}
        )

        qa_system.vector_db = FAISS.from_documents(
            docs,
            embeddings
        )

        # åˆå§‹åŒ–é—®ç­”é“¾
        qa_system.qa_chain = RetrievalQA.from_chain_type(
            llm=qa_system.llm_registry[qa_system.current_model],
            chain_type="stuff",
            retriever=qa_system.vector_db.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
        return True
    except PermissionError as pe:
        logging.error(f"æƒé™æ‹’ç»: {str(pe)}")
        return False
    except Exception as e:
        logging.error(f"æ–‡æ¡£åŠ è½½å¼‚å¸¸: {str(e)}")
        return False
