import os
import time
import torch
import logging
from Core.models import DocumentQASystem
from Core.document_loader import load_document
from Utils.utils import print_help, export_to_excel

def main():
    qa_system = DocumentQASystem()
    print_help()
    current_model = qa_system.current_model

    while True:
        user_input = input("\nYou: ").strip()

        # æŒ‡ä»¤å¤„ç†
        if user_input.startswith("/"):
            if user_input.startswith("/switch"):
                model_name = user_input.split()[-1].lower()
                if model_name in qa_system.llm_registry:
                    qa_system.current_model = model_name
                    qa_system._release_model_resources()
                    qa_system.conversation_chain = None  # å¼ºåˆ¶é‡å»ºå¯¹è¯é“¾
                    print(f"ğŸ”„ å·²åˆ‡æ¢è‡³ {model_name.upper()} æ¨¡å‹")
                    current_model = qa_system.current_model
                else:
                    print(f"âš ï¸ å¯ç”¨æ¨¡å‹ï¼š{list(qa_system.llm_registry.keys())}")
                continue

            elif user_input.startswith("/compare"):
                query = user_input[8:].strip()
                if not query:
                    print("âŒ è¯·è¾“å…¥å¯¹æ¯”é—®é¢˜")
                    continue

                print(f"\nğŸ” æ­£åœ¨å¯¹æ¯”æ¨¡å‹æ€§èƒ½...")
                results = {}
                for name, model in qa_system.llm_registry.items():
                    try:
                        start_time = time.time()
                        response = model.invoke(query)

                        end_time = time.time()
                        latency = end_time - start_time

                        results[name] = {
                            "response": response,
                            "latency": f"{latency:.2f}s"

                        }
                        # é‡Šæ”¾èµ„æº
                        del response
                        qa_system._release_model_resources()
                    except Exception as e:
                        print(f"âŒ {name.upper()} æ¨¡å‹å“åº”å¤±è´¥ï¼š{str(e)}")

                print("\nğŸ†š æ€§èƒ½å¯¹æ¯”ç»“æœï¼š")
                for model, data in results.items():
                    print(f"{model.upper()}:")
                    print(f"â±ï¸ å“åº”æ—¶é—´: {data['latency']}")
                    print(f"ğŸ“ å“åº”ç¤ºä¾‹: {data['response']}...\n")

                export_file = export_to_excel(results, query)
                if export_file:
                    print(f"\nğŸ“Š æ¯”è¾ƒç»“æœå·²å¯¼å‡ºè‡³: {os.path.abspath(export_file)}")
                else:
                    print("\nâš ï¸ å¯¼å‡ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                continue

            elif user_input == "/help":
                print_help()
                continue

            elif user_input == "/upload":
                file_path = input("ğŸ“‚ è¯·è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„: ").strip()
                if not os.path.exists(file_path):
                    print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
                    continue

                if load_document(qa_system, file_path):
                    print("âœ… æ–‡ä»¶å·²åŠ è½½ï¼Œç°åœ¨å¯ä»¥æé—®ï¼")
                continue

            else:
                print("âŒ æœªçŸ¥æŒ‡ä»¤ï¼Œè¯·è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤")
                continue

        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ é€€å‡ºå¯¹è¯")
            break

        # å¦‚æœè¾“å…¥ä¸­åŒ…å«ä¸Šä¼ ç›¸å…³å…³é”®è¯åˆ™é‡å¤ä¸Šä¼ é€»è¾‘
        if any(word in user_input.lower() for word in ["ä¸Šä¼ ", "æ–‡ä»¶", "æ–‡æ¡£", "upload"]):
            file_path = input("ğŸ“‚ è¯·è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„: ").strip()
            if not os.path.exists(file_path):
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
                continue

            if load_document(qa_system, file_path):
                print("âœ… æ–‡ä»¶å·²åŠ è½½ï¼Œç°åœ¨å¯ä»¥æé—®ï¼")
            continue

        # æ„å»ºæˆ–å¤ç”¨å¯¹è¯é“¾
        if not qa_system.conversation_chain:
            from langchain.chains import ConversationChain  # å±€éƒ¨å¯¼å…¥
            qa_system.conversation_chain = ConversationChain(
                llm=qa_system.llm_registry[qa_system.current_model],
                memory=qa_system.memory
            )

        # å¤„ç†é—®ç­”è¯·æ±‚
        try:
            if qa_system.qa_chain:
                result = qa_system.qa_chain({"query": user_input})
                response = f"{result['result']}\n\nğŸ“š æ¥æºæ–‡æ¡£ï¼š{result['source_documents'][0].metadata['source']}"
            else:
                response = qa_system.conversation_chain.predict(input=user_input)

        except Exception as e:
            response = f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}"
            logging.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")

        print(f"{current_model}:", response)

if __name__ == "__main__":
    main()
