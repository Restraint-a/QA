# main.py
import os
import time
import psutil
import Core
import logging
import torch
from langchain.chains import RetrievalQA
from Core.models import DocumentQASystem
from Core.document_loader import load_document
from Core.auto_test import perform_auto_test, visualize_results
from Utils.utils import print_help, export_to_excel, generate_improved_prompt, save_feedback_data

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨æœ€è¿‘çš„æŸ¥è¯¢å’Œå“åº”
last_query = None
last_response = None
last_model = None

def process_command(command: str, qa_system: DocumentQASystem) -> bool:
    """å¤„ç†ç³»ç»Ÿå‘½ä»¤"""
    global last_query, last_response, last_model
    
    command = command.strip().lower()

    if command.startswith("/switch"):
        model_name = command.split()[-1] if len(command.split()) > 1 else ""
        if model_name in qa_system.llm_registry:
            qa_system.current_model = model_name
            qa_system._release_model_resources()
            qa_system.conversation_chain = None
            print(f"å·²åˆ‡æ¢åˆ° {model_name.upper()} æ¨¡å‹")
            return True
        else:
            print(f"âŒ æ— æ•ˆæ¨¡å‹ï¼Œå¯ç”¨é€‰é¡¹ï¼š{list(qa_system.llm_registry.keys())}")
            return False

    # å¤„ç†ç”¨æˆ·åé¦ˆ - èµåŒ
    elif command == "/like":
        if not last_query or not last_response:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ä»¥è¯„ä»·çš„ä¸Šä¸€æ¬¡å¯¹è¯")
            return False
        
        print("âœ… æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼æˆ‘ä»¬ä¼šç»§ç»­ä¿æŒè¿™æ ·çš„å›ç­”è´¨é‡ã€‚")
        save_feedback_data(last_query, last_response, last_model, "like")
        return True
    
    # å¤„ç†ç”¨æˆ·åé¦ˆ - ä¸èµåŒ
    elif command.startswith("/dislike"):
        if not last_query or not last_response:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ä»¥è¯„ä»·çš„ä¸Šä¸€æ¬¡å¯¹è¯")
            return False
        
        # æå–åé¦ˆåŸå› 
        reason = command[8:].strip() if len(command) > 8 else None
        if not reason:
            reason = input("è¯·ç®€å•æè¿°æ‚¨ä¸æ»¡æ„çš„åŸå› : ")
        
        print("ğŸ”„ æ­£åœ¨æ ¹æ®æ‚¨çš„åé¦ˆç”Ÿæˆæ”¹è¿›çš„å›ç­”...")
        
        # ç”Ÿæˆæ”¹è¿›çš„æç¤ºè¯
        improved_prompt = generate_improved_prompt(last_query, last_response, "dislike", reason)
        
        # ä½¿ç”¨æ”¹è¿›çš„æç¤ºè¯é‡æ–°ç”Ÿæˆå›ç­”
        try:
            model = qa_system.llm_registry[qa_system.current_model]
            
            # ç»„åˆåŸå§‹æŸ¥è¯¢å’Œæ”¹è¿›æç¤ºè¯
            combined_prompt = f"{improved_prompt}\n\nåŸå§‹é—®é¢˜: {last_query}"
            
            # é‡æ–°ç”Ÿæˆå›ç­”
            improved_response = model.invoke(combined_prompt)
            
            print(f"\n{qa_system.current_model.upper()} (æ”¹è¿›å):", improved_response)
            
            # ä¿å­˜åé¦ˆå’Œæ”¹è¿›çš„å›ç­”
            save_feedback_data(last_query, last_response, last_model, "dislike", 
                              improved_response, reason)
            
            # æ›´æ–°æœ€è¿‘çš„å“åº”
            last_response = improved_response
            
        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆæ”¹è¿›å›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"
            logging.error(error_msg)
            print(error_msg)
        
        return True

    # main.py ä¸­çš„ /compare å‘½ä»¤å¤„ç†éƒ¨åˆ†
    elif command.startswith("/compare"):
        query_part = command[8:].strip()
        if not query_part:
            print("è¯·è¾“å…¥å¤šè¡ŒæŸ¥è¯¢ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
            lines = []
            while True:
                line = input().rstrip()
                if line == "":
                    break
                lines.append(line)
            query = "\n".join(lines)
        else:
            query = query_part

        if not query:
            print("è¯·æä¾›æŸ¥è¯¢å†…å®¹ï¼Œæ ¼å¼ï¼š/compare [æŸ¥è¯¢å†…å®¹]")
            return False

        print(f"\næ­£åœ¨æ¯”è¾ƒå„æ¨¡å‹çš„å“åº”...")
        results = {}
        for name, model in qa_system.llm_registry.items():
            try:
                # å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ç­‰å¾…ç³»ç»Ÿç¨³å®š
                time.sleep(2)
                
                # è®°å½•å¼€å§‹æ—¶çš„å†…å­˜å’Œæ˜¾å­˜ä½¿ç”¨
                process = psutil.Process(os.getpid())
                memory_before = process.memory_info().rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
                
                # è®°å½•å¼€å§‹æ—¶çš„GPUæ˜¾å­˜ä½¿ç”¨
                gpu_memory_before = qa_system.get_gpu_memory_usage()
                
                start_time = time.time()
                if qa_system.qa_chain:  # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£ä¸Šä¼ 
                    # ä½¿ç”¨ qa_chain ç”ŸæˆåŸºäºæ–‡æ¡£çš„å›ç­”
                    qa_system.qa_chain = RetrievalQA.from_chain_type(
                        llm=model,
                        chain_type="stuff",
                        retriever=qa_system.vector_db.as_retriever(search_kwargs={"k": 3}),
                        return_source_documents=True
                    )
                    result = qa_system.qa_chain({"query": query})
                    response = f"{result['result']}\nğŸ“š æ¥æºï¼š{result['source_documents'][0].metadata['source']}"
                else:
                    # æ²¡æœ‰æ–‡æ¡£ä¸Šä¼ ï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹
                    response = model.invoke(query)
                end_time = time.time()
                
                # è®°å½•ç»“æŸæ—¶çš„å†…å­˜ä½¿ç”¨
                memory_after = process.memory_info().rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
                memory_usage = memory_after - memory_before
                
                # ç­‰å¾…ç³»ç»Ÿç¨³å®šåå†æµ‹é‡æ˜¾å­˜
                time.sleep(2)
                
                # å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                # è®°å½•ç»“æŸæ—¶çš„GPUæ˜¾å­˜ä½¿ç”¨
                gpu_memory_after = qa_system.get_gpu_memory_usage()
                
                # è®¡ç®—GPUæ˜¾å­˜å˜åŒ–
                gpu_memory_diff = {}
                if gpu_memory_before["available"] and gpu_memory_after["available"]:
                    for device_id, before_stats in gpu_memory_before["devices"].items():
                        after_stats = gpu_memory_after["devices"][device_id]
                        
                        # åªè®¡ç®—æ˜¾å­˜ä½¿ç”¨å˜åŒ–ï¼Œä¸å†è®¡ç®—åˆ©ç”¨ç‡
                        memory_diff = round(after_stats["used_memory_mb"] - before_stats["used_memory_mb"], 2)
                        
                        # è·å–å³°å€¼æ˜¾å­˜ä½¿ç”¨
                        peak_memory = after_stats.get("peak_memory_mb", after_stats["used_memory_mb"])
                        
                        gpu_memory_diff[device_id] = {
                            "device_name": before_stats["device_name"],
                            "memory_diff_mb": memory_diff,
                            "peak_memory_mb": peak_memory
                        }
                # åˆ é™¤ä»¥ä¸‹ä¸æ˜¾å­˜åˆ©ç”¨ç‡ç›¸å…³çš„ä»£ç 
                # util_before = round(before_stats["utilization_percent"], 2)
                # util_after = round(after_stats["utilization_percent"], 2)
                
                # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡ - æ˜¾å­˜ä½¿ç”¨ä¸åˆ©ç”¨ç‡å˜åŒ–æ˜¯å¦ä¸€è‡´
                # memory_change_direction = 1 if memory_diff > 0 else (-1 if memory_diff < 0 else 0)
                # util_change_direction = 1 if (util_after - util_before) > 0 else (-1 if (util_after - util_before) < 0 else 0)
                # consistency = "ä¸€è‡´" if memory_change_direction == util_change_direction else "ä¸ä¸€è‡´"
                
                # gpu_memory_diff[device_id] = {
                #     "device_name": before_stats["device_name"],
                #     "memory_diff_mb": memory_diff,
                #     "utilization_before": util_before,
                #     "utilization_after": util_after,
                #     "utilization_diff": round(util_after - util_before, 2),
                #     "consistency": consistency
                # }
                
                latency = end_time - start_time
                
                # ä¼°ç®—ä»¤ç‰Œæ•°é‡ (ç®€å•ä¼°è®¡ï¼Œæ¯ä¸ªå•è¯çº¦1.3ä¸ªä»¤ç‰Œ)
                response_words = len(response.split())
                estimated_tokens = int(response_words * 1.3)
                
                # è®¡ç®—ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦
                tokens_per_second = estimated_tokens / latency if latency > 0 else 0
                
                results[name] = {
                    "response": response,
                    "latency": f"{latency:.2f}s",
                    "tokens": estimated_tokens,
                    "tokens_per_second": f"{tokens_per_second:.2f}",
                    "response_length": len(response),
                    "memory_usage": f"{memory_usage:.2f}",
                    "gpu_memory_diff": gpu_memory_diff
                }
                
                # é‡Šæ”¾èµ„æº
                del response
                qa_system._release_model_resources()
            except Exception as e:
                print(f"{name.upper()} è°ƒç”¨å‡ºé”™ï¼š{str(e)}")

        print("\næ¯”è¾ƒç»“æœï¼š")
        for model_name, data in results.items():
            print(f"{model_name.upper()}:")
            print(f"å»¶è¿Ÿï¼š{data['latency']}")
            print(f"ä¼°è®¡ä»¤ç‰Œæ•°ï¼š{data['tokens']}")
            print(f"ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦ï¼š{data['tokens_per_second']} tokens/s")
            print(f"å“åº”é•¿åº¦ï¼š{data['response_length']} å­—ç¬¦")
            print(f"å†…å­˜ä½¿ç”¨ï¼š{data['memory_usage']} MB")
            
            # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if "gpu_memory_diff" in data and data["gpu_memory_diff"]:
                print("GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ:")
                for device_id, gpu_stats in data["gpu_memory_diff"].items():
                    print(f"  {gpu_stats['device_name']}:")
                    print(f"    æ˜¾å­˜ä½¿ç”¨å˜åŒ–: {gpu_stats['memory_diff_mb']} MB")
                    if "peak_memory_mb" in gpu_stats:
                        print(f"    æ˜¾å­˜ä½¿ç”¨å³°å€¼: {gpu_stats['peak_memory_mb']} MB")
            else:
                print("GPUæ˜¾å­˜ä¿¡æ¯: ä¸å¯ç”¨")
            
            print(f"å“åº”é¢„è§ˆï¼š{data['response'][:100]}...\n")

        export_file = export_to_excel(results, query)
        if export_file:
            print(f"\nå·²å¯¼å‡ºç»“æœè‡³ï¼š{os.path.abspath(export_file)}")
        else:
            print("âŒ \nå¯¼å‡ºç»“æœå¤±è´¥")
        return True

    # /autotest - è‡ªåŠ¨æµ‹è¯•å‘½ä»¤
    elif command.startswith("/autotest"):
        # è¯¢é—®æµ‹è¯•æ•°é‡
        logic_count = int(input("æµ‹è¯•çš„é€»è¾‘é¢˜æ•°é‡ï¼š"))
        read_count = int(input("æµ‹è¯•çš„é˜…è¯»ç†è§£é¢˜æ•°é‡ï¼š"))
        math_count = int(input("æµ‹è¯•çš„æ•°å­¦é¢˜æ•°é‡ï¼š"))

        # è°ƒç”¨ç›¸å…³å‡½æ•°åŠ è½½é—®é¢˜å¹¶è¿›è¡Œæµ‹è¯•
        print(f"å¼€å§‹è‡ªåŠ¨æµ‹è¯• {logic_count} é“é€»è¾‘é¢˜ï¼Œ{read_count} é“é˜…è¯»ç†è§£é¢˜ï¼Œ{math_count} é“æ•°å­¦é¢˜...")
        results, standard_answers = perform_auto_test(qa_system, logic_count, read_count, math_count)

        # å¯¼å‡ºå¹¶å¯è§†åŒ–ç»“æœ
        export_file = Core.auto_test.export_to_excel(results, standard_answers)
        if export_file:
            print(f"\næµ‹è¯•ç»“æœå·²å¯¼å‡ºè‡³ï¼š{os.path.abspath(export_file)}")
            visualize_results(export_file)  # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        else:
            print("âŒ å¯¼å‡ºç»“æœå¤±è´¥")

        return True

    elif command == "/help":
        print_help()
        return True

    elif command == "/upload":
        file_path = input("ğŸ“‚ è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼š").strip()
        if not os.path.exists(file_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        if load_document(qa_system, file_path):
            print("ğŸ“„  æ–‡æ¡£åŠ è½½æˆåŠŸ")
            return True
        print("âŒ æ–‡æ¡£åŠ è½½å¤±è´¥")
        return False

    else:
        print("âŒ æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥/helpæŸ¥çœ‹å¸®åŠ©")
        return False

def process_query(query: str, qa_system: DocumentQASystem, current_model: str) -> None:
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    global last_query, last_response, last_model
    
    try:
        if qa_system.qa_chain:
            result = qa_system.qa_chain({"query": query})
            response = f"{result['result']}\næ¥æºï¼š{result['source_documents'][0].metadata['source']}"
        else:
            if not qa_system.conversation_chain:
                from langchain.chains import ConversationChain
                qa_system.conversation_chain = ConversationChain(
                    llm=qa_system.llm_registry[qa_system.current_model],
                    memory=qa_system.memory
                )
            response = qa_system.conversation_chain.predict(input=query)

        print(f"\n{current_model.upper()}:", response)
        
        # ä¿å­˜æœ€è¿‘çš„æŸ¥è¯¢å’Œå“åº”ï¼Œä»¥ä¾¿åé¦ˆ
        last_query = query
        last_response = response
        last_model = current_model
        
        # æç¤ºç”¨æˆ·å¯ä»¥æä¾›åé¦ˆ
        print("\nğŸ’¬ æ‚¨å¯ä»¥ä½¿ç”¨ /like è¡¨ç¤ºèµåŒï¼Œæˆ– /dislike [åŸå› ] è¡¨ç¤ºä¸èµåŒ")

    except Exception as e:
        error_msg = f"âŒ å¤„ç†é”™è¯¯ï¼š{str(e)}"
        logging.error(error_msg)
        print(error_msg)

def main():
    qa_system = DocumentQASystem()
    print_help()
    current_model = qa_system.current_model
    
    # ç¡®ä¿åé¦ˆç›®å½•å­˜åœ¨
    os.makedirs("./feedback_data", exist_ok=True)

    while True:
        try:
            # åˆå§‹åŒ–è¾“å…¥æ”¶é›†
            user_input = []
            print("\nYou: (è¾“å…¥å†…å®¹ï¼Œè¿æŒ‰ä¸¤æ¬¡å›è½¦æäº¤)")

            # å¤šè¡Œè¾“å…¥å¾ªç¯
            while True:
                line = input().strip()

                # é€€å‡ºæŒ‡ä»¤å¤„ç†
                if line.lower() in ["exit", "quit","/exit","/quit"]:
                    print("ğŸ‘‹  å†è§ï¼")
                    return

                # å‘½ä»¤ç«‹å³æ‰§è¡Œ
                if line.startswith("/"):
                    process_command(line, qa_system)
                    current_model = qa_system.current_model
                    break

                # ç©ºè¡Œè¡¨ç¤ºæäº¤è¾“å…¥
                if not line:
                    if user_input:
                        full_query = "\n".join(user_input)
                        print("ğŸ¤– Modelæ€è€ƒä¸­......")
                        process_query(full_query, qa_system, current_model)
                    user_input = []
                    break

                user_input.append(line)

        except KeyboardInterrupt:
            print("\nè¾“å…¥ä¸­æ–­ï¼Œè¾“å…¥ exit é€€å‡ºç¨‹åº")
        except Exception as e:
            logging.error(f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}")
            print("âŒ å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•")

if __name__ == "__main__":
    main()