{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# å±è”½ä¸å¿…è¦çš„è­¦å‘Š\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# æ—¥å¿—é…ç½®\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# ç¯å¢ƒå˜é‡è®¾ç½®\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "993874fedeac8cf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# document_loader.py\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import chardet\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def detect_encoding(file_path, max_size=1024*1024):\n",
    "    \"\"\"å®‰å…¨æ£€æµ‹æ–‡æœ¬æ–‡ä»¶ç¼–ç \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = b''\n",
    "            chunk_size = 4096\n",
    "            while len(raw_data) < max_size:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                raw_data += chunk\n",
    "            return chardet.detect(raw_data)['encoding']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def load_document(qa_system, file_path):\n",
    "    \"\"\"æ–‡æ¡£åŠ è½½å¤„ç†\"\"\"\n",
    "    print(f\"ğŸ“„  è¯»å–æ–‡ä»¶ï¼š{file_path}\")\n",
    "    try:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise ValueError(\"è·¯å¾„ä¸æ˜¯æ–‡ä»¶\")\n",
    "\n",
    "        # ç¼–ç æ£€æµ‹å’Œå¤‡é€‰ç¼–ç åˆ—è¡¨\n",
    "        encoding = detect_encoding(file_path)\n",
    "        retry_encodings = [encoding, 'utf-8', 'gbk']\n",
    "\n",
    "        loader = None\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.lower().endswith(('.docx', '.doc')):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            for enc in retry_encodings:\n",
    "                try:\n",
    "                    loader = TextLoader(file_path, encoding=enc)\n",
    "                    loader.load()  # æµ‹è¯•åŠ è½½\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "        documents = loader.load()\n",
    "\n",
    "        # ä½¿ç”¨é€’å½’æ–‡æœ¬åˆ†å‰²å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡åˆ†\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"â€¦â€¦\", \"â€¦\", \"ã€€\"]\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æœ¬å—\")\n",
    "        if docs:\n",
    "            print(f\"ğŸ“ é¦–æ–‡æœ¬å—ç¤ºä¾‹ï¼š{docs[0].page_content[:200]}...\")\n",
    "            #print(f\"ğŸ“ æ–‡æœ¬ï¼š{docs}\")\n",
    "\n",
    "        # æ„é€ å‘é‡æ•°æ®åº“\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"batch_size\": 32}\n",
    "        )\n",
    "\n",
    "        qa_system.vector_db = FAISS.from_documents(\n",
    "            docs,\n",
    "            embeddings\n",
    "        )\n",
    "\n",
    "        # åˆå§‹åŒ–é—®ç­”é“¾\n",
    "        qa_system.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=qa_system.llm_registry[qa_system.current_model],\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=qa_system.vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        return True\n",
    "    except PermissionError as pe:\n",
    "        logging.error(f\"âŒ æƒé™æ‹’ç»: {str(pe)}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ æ–‡æ¡£åŠ è½½å¼‚å¸¸: {str(e)}\")\n",
    "        return False\n"
   ],
   "id": "d11219aba8a72c88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# models.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "class DocumentQASystem:\n",
    "    def __init__(self):\n",
    "        self.llm_registry = self._init_models()\n",
    "        self.current_model = \"mistral\"\n",
    "        self.memory = ConversationBufferMemory()\n",
    "        self.vector_db = None\n",
    "        self.qa_chain = None\n",
    "        self.conversation_chain = None\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ¨¡å‹å®ä¾‹å¹¶ç¼“å­˜\"\"\"\n",
    "        return {\n",
    "            \"mistral\": Ollama(\n",
    "                model=\"mistral\",\n",
    "                temperature=0.7,\n",
    "                num_ctx=4096,\n",
    "            ),\n",
    "            \"qwen\": Ollama(\n",
    "                model=\"qwen2.5:7b\",\n",
    "                temperature=0.5,\n",
    "                num_ctx=2048,\n",
    "            ),\n",
    "            \"deepseek\": Ollama(\n",
    "                model=\"deepseek-r1:7b\",\n",
    "                temperature=0.5,\n",
    "                num_ctx=2048,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _release_model_resources(self):\n",
    "        \"\"\"å½»åº•é‡Šæ”¾æ¨¡å‹èµ„æº\"\"\"\n",
    "        for model in self.llm_registry.values():\n",
    "            if hasattr(model, 'client'):\n",
    "                del model.client\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n"
   ],
   "id": "9949bf4fe91fa7cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check_env.py\n",
    "import sys, torch, numpy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"[ç³»ç»Ÿä¿¡æ¯]\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"numpyç‰ˆæœ¬: {numpy.__version__}\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"æ‰€ç”¨è®¾å¤‡ï¼š{torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n[å…³é”®åŠŸèƒ½éªŒè¯]\")\n",
    "\n",
    "try:\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    print(\"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    test_text = \"æµ‹è¯•ä¸­æ–‡å‘é‡åŒ–\"\n",
    "    embedding = emb.embed_query(test_text)\n",
    "    print(f\"å‘é‡ç»´åº¦: {len(embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\")"
   ],
   "id": "c96d6065345c6a29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# utils.py\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "DEFAULT_PATH = \"./compare_output\"\n",
    "def print_help():\n",
    "    print(\"\"\"\n",
    "ğŸ”§ ç³»ç»ŸæŒ‡ä»¤æ‰‹å†Œï¼š\n",
    "    /switch [mistral|qwen|deepseek]  - åˆ‡æ¢å¤§è¯­è¨€æ¨¡å‹\n",
    "    /compare [é—®é¢˜]         - å¯¹æ¯”æ¨¡å‹æ€§èƒ½\n",
    "    /help                  - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯\n",
    "    /upload                - ä¸Šä¼ æ–‡æ¡£è¿›å…¥é—®ç­”æ¨¡å¼\n",
    "    exit/quit              - é€€å‡ºç³»ç»Ÿ\n",
    "\"\"\")\n",
    "\n",
    "def export_to_excel(results, query):\n",
    "    try:\n",
    "        safe_query = query.replace(\"\\n\", \" \")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"model_comparison_{timestamp}.xlsx\"\n",
    "\n",
    "        # ç»„è£…æ•°æ®\n",
    "        data = []\n",
    "        for model_name, result in results.items():\n",
    "            row = {\n",
    "                \"Model\": model_name.upper(),\n",
    "                \"Query\": safe_query,\n",
    "                \"Latency\": result[\"latency\"],\n",
    "                \"Response Preview\": result[\"response\"]\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "        # æ„é€  DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[[\"Model\", \"Query\", \"Latency\", \"Response Preview\"]]\n",
    "\n",
    "        filename = os.path.join(DEFAULT_PATH, filename)\n",
    "\n",
    "        # å¯¼å‡º Excel æ–‡ä»¶\n",
    "        df.to_excel(filename, index=False, engine=\"openpyxl\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        logging.error(f\"å¯¼å‡ºå¤±è´¥: {str(e)}\")\n",
    "        return None"
   ],
   "id": "de99e96a28c2dd99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "430e43b3a4dc979c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# main.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from Core.models import DocumentQASystem\n",
    "from Core.document_loader import load_document\n",
    "from Utils.utils import print_help, export_to_excel\n",
    "\n",
    "def process_command(command: str, qa_system: DocumentQASystem) -> bool:\n",
    "    \"\"\"å¤„ç†ç³»ç»Ÿå‘½ä»¤\"\"\"\n",
    "    command = command.strip().lower()\n",
    "\n",
    "    if command.startswith(\"/switch\"):\n",
    "        model_name = command.split()[-1] if len(command.split()) > 1 else \"\"\n",
    "        if model_name in qa_system.llm_registry:\n",
    "            qa_system.current_model = model_name\n",
    "            qa_system._release_model_resources()\n",
    "            qa_system.conversation_chain = None\n",
    "            print(f\"å·²åˆ‡æ¢åˆ° {model_name.upper()} æ¨¡å‹\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ æ— æ•ˆæ¨¡å‹ï¼Œå¯ç”¨é€‰é¡¹ï¼š{list(qa_system.llm_registry.keys())}\")\n",
    "            return False\n",
    "\n",
    "    # main.py ä¸­çš„ process_command å‡½æ•°éƒ¨åˆ†\n",
    "    elif command.startswith(\"/compare\"):\n",
    "        query_part = command[8:].strip()\n",
    "        if not query_part:\n",
    "            print(\"è¯·è¾“å…¥å¤šè¡ŒæŸ¥è¯¢ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š\")\n",
    "            lines = []\n",
    "            while True:\n",
    "                line = input().rstrip()\n",
    "                if line == \"\":\n",
    "                    break\n",
    "                lines.append(line)\n",
    "            query = \"\\n\".join(lines)\n",
    "        else:\n",
    "            query = query_part\n",
    "\n",
    "        if not query:\n",
    "            print(\"è¯·æä¾›æŸ¥è¯¢å†…å®¹ï¼Œæ ¼å¼ï¼š/compare [æŸ¥è¯¢å†…å®¹]\")\n",
    "            return False\n",
    "\n",
    "        print(f\"\\næ­£åœ¨æ¯”è¾ƒå„æ¨¡å‹çš„å“åº”...\")\n",
    "        results = {}\n",
    "        for name, model in qa_system.llm_registry.items():\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                if qa_system.qa_chain:  # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£ä¸Šä¼ \n",
    "                    # ä½¿ç”¨ qa_chain ç”ŸæˆåŸºäºæ–‡æ¡£çš„å›ç­”\n",
    "                    qa_system.qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm=model,\n",
    "                        chain_type=\"stuff\",\n",
    "                        retriever=qa_system.vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                        return_source_documents=True\n",
    "                    )\n",
    "                    result = qa_system.qa_chain({\"query\": query})\n",
    "                    response = f\"{result['result']}\\nğŸ“š æ¥æºï¼š{result['source_documents'][0].metadata['source']}\"\n",
    "                else:\n",
    "                    # æ²¡æœ‰æ–‡æ¡£ä¸Šä¼ ï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹\n",
    "                    response = model.invoke(query)\n",
    "                end_time = time.time()\n",
    "                latency = end_time - start_time\n",
    "                results[name] = {\n",
    "                    \"response\": response,\n",
    "                    \"latency\": f\"{latency:.2f}s\"\n",
    "                }\n",
    "                # é‡Šæ”¾èµ„æº\n",
    "                del response\n",
    "                qa_system._release_model_resources()\n",
    "            except Exception as e:\n",
    "                print(f\"{name.upper()} è°ƒç”¨å‡ºé”™ï¼š{str(e)}\")\n",
    "\n",
    "        print(\"\\næ¯”è¾ƒç»“æœï¼š\")\n",
    "        for model_name, data in results.items():\n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(f\"å»¶è¿Ÿï¼š{data['latency']}\")\n",
    "            print(f\"å“åº”é¢„è§ˆï¼š{data['response'][:200]}...\\n\")\n",
    "\n",
    "        export_file = export_to_excel(results, query)\n",
    "        if export_file:\n",
    "            print(f\"\\nå·²å¯¼å‡ºç»“æœè‡³ï¼š{os.path.abspath(export_file)}\")\n",
    "        else:\n",
    "            print(\"âŒ \\nå¯¼å‡ºç»“æœå¤±è´¥\")\n",
    "        return True\n",
    "\n",
    "    elif command == \"/help\":\n",
    "        print_help()\n",
    "        return True\n",
    "\n",
    "    elif command == \"/upload\":\n",
    "        file_path = input(\"ğŸ“‚ è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼š\").strip()\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"âŒ æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "            return False\n",
    "        if load_document(qa_system, file_path):\n",
    "            print(\"ğŸ“„  æ–‡æ¡£åŠ è½½æˆåŠŸ\")\n",
    "            return True\n",
    "        print(\"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥\")\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥/helpæŸ¥çœ‹å¸®åŠ©\")\n",
    "        return False\n",
    "\n",
    "def process_query(query: str, qa_system: DocumentQASystem, current_model: str) -> None:\n",
    "    \"\"\"å¤„ç†ç”¨æˆ·æŸ¥è¯¢\"\"\"\n",
    "    try:\n",
    "        if qa_system.qa_chain:\n",
    "            result = qa_system.qa_chain({\"query\": query})\n",
    "            response = f\"{result['result']}\\næ¥æºï¼š{result['source_documents'][0].metadata['source']}\"\n",
    "        else:\n",
    "            if not qa_system.conversation_chain:\n",
    "                from langchain.chains import ConversationChain\n",
    "                qa_system.conversation_chain = ConversationChain(\n",
    "                    llm=qa_system.llm_registry[qa_system.current_model],\n",
    "                    memory=qa_system.memory\n",
    "                )\n",
    "            response = qa_system.conversation_chain.predict(input=query)\n",
    "\n",
    "        print(f\"\\n{current_model.upper()}:\", response)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ å¤„ç†é”™è¯¯ï¼š{str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "\n",
    "def main():\n",
    "    qa_system = DocumentQASystem()\n",
    "    print_help()\n",
    "    current_model = qa_system.current_model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # åˆå§‹åŒ–è¾“å…¥æ”¶é›†\n",
    "            user_input = []\n",
    "            print(\"\\nYou: (è¾“å…¥å†…å®¹ï¼Œè¿æŒ‰ä¸¤æ¬¡å›è½¦æäº¤)\")\n",
    "\n",
    "            # å¤šè¡Œè¾“å…¥å¾ªç¯\n",
    "            while True:\n",
    "                line = input().strip()\n",
    "\n",
    "                # é€€å‡ºæŒ‡ä»¤å¤„ç†\n",
    "                if line.lower() in [\"exit\", \"quit\"]:\n",
    "                    print(\"ğŸ‘‹  å†è§ï¼\")\n",
    "                    return\n",
    "\n",
    "                # å‘½ä»¤ç«‹å³æ‰§è¡Œ\n",
    "                if line.startswith(\"/\"):\n",
    "                    process_command(line, qa_system)\n",
    "                    current_model = qa_system.current_model\n",
    "                    break\n",
    "\n",
    "                # ç©ºè¡Œè¡¨ç¤ºæäº¤è¾“å…¥\n",
    "                if not line:\n",
    "                    if user_input:\n",
    "                        full_query = \"\\n\".join(user_input)\n",
    "                        print(\"ğŸ¤– Modelæ€è€ƒä¸­......\")\n",
    "                        process_query(full_query, qa_system, current_model)\n",
    "                    user_input = []\n",
    "                    break\n",
    "\n",
    "                user_input.append(line)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nè¾“å…¥ä¸­æ–­ï¼Œè¾“å…¥ exit é€€å‡ºç¨‹åº\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}\")\n",
    "            print(\"âŒ å‘ç”Ÿæ„å¤–é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "81174f21210bc103"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
