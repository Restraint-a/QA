{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# 屏蔽不必要的警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 日志配置\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# 环境变量设置\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "993874fedeac8cf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# document_loader.py\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import chardet\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def detect_encoding(file_path, max_size=1024*1024):\n",
    "    \"\"\"安全检测文本文件编码\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = b''\n",
    "            chunk_size = 4096\n",
    "            while len(raw_data) < max_size:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                raw_data += chunk\n",
    "            return chardet.detect(raw_data)['encoding']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"编码检测失败: {str(e)}\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def load_document(qa_system, file_path):\n",
    "    \"\"\"文档加载处理\"\"\"\n",
    "    print(f\"📄  读取文件：{file_path}\")\n",
    "    try:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise ValueError(\"路径不是文件\")\n",
    "\n",
    "        # 编码检测和备选编码列表\n",
    "        encoding = detect_encoding(file_path)\n",
    "        retry_encodings = [encoding, 'utf-8', 'gbk']\n",
    "\n",
    "        loader = None\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.lower().endswith(('.docx', '.doc')):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            for enc in retry_encodings:\n",
    "                try:\n",
    "                    loader = TextLoader(file_path, encoding=enc)\n",
    "                    loader.load()  # 测试加载\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "        documents = loader.load()\n",
    "\n",
    "        # 使用递归文本分割器对文档进行切分\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\", \"……\", \"…\", \"　\"]\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        print(f\"✅ 成功加载 {len(docs)} 个文本块\")\n",
    "        if docs:\n",
    "            print(f\"📝 首文本块示例：{docs[0].page_content[:200]}...\")\n",
    "            #print(f\"📝 文本：{docs}\")\n",
    "\n",
    "        # 构造向量数据库\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"batch_size\": 32}\n",
    "        )\n",
    "\n",
    "        qa_system.vector_db = FAISS.from_documents(\n",
    "            docs,\n",
    "            embeddings\n",
    "        )\n",
    "\n",
    "        # 初始化问答链\n",
    "        qa_system.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=qa_system.llm_registry[qa_system.current_model],\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=qa_system.vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        return True\n",
    "    except PermissionError as pe:\n",
    "        logging.error(f\"❌ 权限拒绝: {str(pe)}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ 文档加载异常: {str(e)}\")\n",
    "        return False\n"
   ],
   "id": "d11219aba8a72c88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# models.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "class DocumentQASystem:\n",
    "    def __init__(self):\n",
    "        self.llm_registry = self._init_models()\n",
    "        self.current_model = \"mistral\"\n",
    "        self.memory = ConversationBufferMemory()\n",
    "        self.vector_db = None\n",
    "        self.qa_chain = None\n",
    "        self.conversation_chain = None\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"初始化模型实例并缓存\"\"\"\n",
    "        return {\n",
    "            \"mistral\": Ollama(\n",
    "                model=\"mistral\",\n",
    "                temperature=0.7,\n",
    "                num_ctx=4096,\n",
    "            ),\n",
    "            \"qwen\": Ollama(\n",
    "                model=\"qwen2.5:7b\",\n",
    "                temperature=0.5,\n",
    "                num_ctx=2048,\n",
    "            ),\n",
    "            \"deepseek\": Ollama(\n",
    "                model=\"deepseek-r1:7b\",\n",
    "                temperature=0.5,\n",
    "                num_ctx=2048,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _release_model_resources(self):\n",
    "        \"\"\"彻底释放模型资源\"\"\"\n",
    "        for model in self.llm_registry.values():\n",
    "            if hasattr(model, 'client'):\n",
    "                del model.client\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n"
   ],
   "id": "9949bf4fe91fa7cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check_env.py\n",
    "import sys, torch, numpy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"[系统信息]\")\n",
    "print(f\"Python版本: {sys.version}\")\n",
    "print(f\"numpy版本: {numpy.__version__}\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "print(f\"所用设备：{torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n[关键功能验证]\")\n",
    "\n",
    "try:\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    print(\"✅ 嵌入模型初始化成功\")\n",
    "    test_text = \"测试中文向量化\"\n",
    "    embedding = emb.embed_query(test_text)\n",
    "    print(f\"向量维度: {len(embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 初始化失败: {str(e)}\")"
   ],
   "id": "c96d6065345c6a29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# utils.py\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "DEFAULT_PATH = \"./compare_output\"\n",
    "def print_help():\n",
    "    print(\"\"\"\n",
    "🔧 系统指令手册：\n",
    "    /switch [mistral|qwen|deepseek]  - 切换大语言模型\n",
    "    /compare [问题]         - 对比模型性能\n",
    "    /help                  - 显示帮助信息\n",
    "    /upload                - 上传文档进入问答模式\n",
    "    exit/quit              - 退出系统\n",
    "\"\"\")\n",
    "\n",
    "def export_to_excel(results, query):\n",
    "    try:\n",
    "        safe_query = query.replace(\"\\n\", \" \")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"model_comparison_{timestamp}.xlsx\"\n",
    "\n",
    "        # 组装数据\n",
    "        data = []\n",
    "        for model_name, result in results.items():\n",
    "            row = {\n",
    "                \"Model\": model_name.upper(),\n",
    "                \"Query\": safe_query,\n",
    "                \"Latency\": result[\"latency\"],\n",
    "                \"Response Preview\": result[\"response\"]\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "        # 构造 DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df[[\"Model\", \"Query\", \"Latency\", \"Response Preview\"]]\n",
    "\n",
    "        filename = os.path.join(DEFAULT_PATH, filename)\n",
    "\n",
    "        # 导出 Excel 文件\n",
    "        df.to_excel(filename, index=False, engine=\"openpyxl\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        logging.error(f\"导出失败: {str(e)}\")\n",
    "        return None"
   ],
   "id": "de99e96a28c2dd99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "430e43b3a4dc979c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# main.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from Core.models import DocumentQASystem\n",
    "from Core.document_loader import load_document\n",
    "from Utils.utils import print_help, export_to_excel\n",
    "\n",
    "def process_command(command: str, qa_system: DocumentQASystem) -> bool:\n",
    "    \"\"\"处理系统命令\"\"\"\n",
    "    command = command.strip().lower()\n",
    "\n",
    "    if command.startswith(\"/switch\"):\n",
    "        model_name = command.split()[-1] if len(command.split()) > 1 else \"\"\n",
    "        if model_name in qa_system.llm_registry:\n",
    "            qa_system.current_model = model_name\n",
    "            qa_system._release_model_resources()\n",
    "            qa_system.conversation_chain = None\n",
    "            print(f\"已切换到 {model_name.upper()} 模型\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ 无效模型，可用选项：{list(qa_system.llm_registry.keys())}\")\n",
    "            return False\n",
    "\n",
    "    # main.py 中的 process_command 函数部分\n",
    "    elif command.startswith(\"/compare\"):\n",
    "        query_part = command[8:].strip()\n",
    "        if not query_part:\n",
    "            print(\"请输入多行查询（输入空行结束）：\")\n",
    "            lines = []\n",
    "            while True:\n",
    "                line = input().rstrip()\n",
    "                if line == \"\":\n",
    "                    break\n",
    "                lines.append(line)\n",
    "            query = \"\\n\".join(lines)\n",
    "        else:\n",
    "            query = query_part\n",
    "\n",
    "        if not query:\n",
    "            print(\"请提供查询内容，格式：/compare [查询内容]\")\n",
    "            return False\n",
    "\n",
    "        print(f\"\\n正在比较各模型的响应...\")\n",
    "        results = {}\n",
    "        for name, model in qa_system.llm_registry.items():\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                if qa_system.qa_chain:  # 检查是否有文档上传\n",
    "                    # 使用 qa_chain 生成基于文档的回答\n",
    "                    qa_system.qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm=model,\n",
    "                        chain_type=\"stuff\",\n",
    "                        retriever=qa_system.vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                        return_source_documents=True\n",
    "                    )\n",
    "                    result = qa_system.qa_chain({\"query\": query})\n",
    "                    response = f\"{result['result']}\\n📚 来源：{result['source_documents'][0].metadata['source']}\"\n",
    "                else:\n",
    "                    # 没有文档上传，直接调用模型\n",
    "                    response = model.invoke(query)\n",
    "                end_time = time.time()\n",
    "                latency = end_time - start_time\n",
    "                results[name] = {\n",
    "                    \"response\": response,\n",
    "                    \"latency\": f\"{latency:.2f}s\"\n",
    "                }\n",
    "                # 释放资源\n",
    "                del response\n",
    "                qa_system._release_model_resources()\n",
    "            except Exception as e:\n",
    "                print(f\"{name.upper()} 调用出错：{str(e)}\")\n",
    "\n",
    "        print(\"\\n比较结果：\")\n",
    "        for model_name, data in results.items():\n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(f\"延迟：{data['latency']}\")\n",
    "            print(f\"响应预览：{data['response'][:200]}...\\n\")\n",
    "\n",
    "        export_file = export_to_excel(results, query)\n",
    "        if export_file:\n",
    "            print(f\"\\n已导出结果至：{os.path.abspath(export_file)}\")\n",
    "        else:\n",
    "            print(\"❌ \\n导出结果失败\")\n",
    "        return True\n",
    "\n",
    "    elif command == \"/help\":\n",
    "        print_help()\n",
    "        return True\n",
    "\n",
    "    elif command == \"/upload\":\n",
    "        file_path = input(\"📂 请输入文件路径：\").strip()\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"❌ 文件不存在\")\n",
    "            return False\n",
    "        if load_document(qa_system, file_path):\n",
    "            print(\"📄  文档加载成功\")\n",
    "            return True\n",
    "        print(\"❌ 文档加载失败\")\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        print(\"❌ 未知命令，输入/help查看帮助\")\n",
    "        return False\n",
    "\n",
    "def process_query(query: str, qa_system: DocumentQASystem, current_model: str) -> None:\n",
    "    \"\"\"处理用户查询\"\"\"\n",
    "    try:\n",
    "        if qa_system.qa_chain:\n",
    "            result = qa_system.qa_chain({\"query\": query})\n",
    "            response = f\"{result['result']}\\n来源：{result['source_documents'][0].metadata['source']}\"\n",
    "        else:\n",
    "            if not qa_system.conversation_chain:\n",
    "                from langchain.chains import ConversationChain\n",
    "                qa_system.conversation_chain = ConversationChain(\n",
    "                    llm=qa_system.llm_registry[qa_system.current_model],\n",
    "                    memory=qa_system.memory\n",
    "                )\n",
    "            response = qa_system.conversation_chain.predict(input=query)\n",
    "\n",
    "        print(f\"\\n{current_model.upper()}:\", response)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ 处理错误：{str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "\n",
    "def main():\n",
    "    qa_system = DocumentQASystem()\n",
    "    print_help()\n",
    "    current_model = qa_system.current_model\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 初始化输入收集\n",
    "            user_input = []\n",
    "            print(\"\\nYou: (输入内容，连按两次回车提交)\")\n",
    "\n",
    "            # 多行输入循环\n",
    "            while True:\n",
    "                line = input().strip()\n",
    "\n",
    "                # 退出指令处理\n",
    "                if line.lower() in [\"exit\", \"quit\"]:\n",
    "                    print(\"👋  再见！\")\n",
    "                    return\n",
    "\n",
    "                # 命令立即执行\n",
    "                if line.startswith(\"/\"):\n",
    "                    process_command(line, qa_system)\n",
    "                    current_model = qa_system.current_model\n",
    "                    break\n",
    "\n",
    "                # 空行表示提交输入\n",
    "                if not line:\n",
    "                    if user_input:\n",
    "                        full_query = \"\\n\".join(user_input)\n",
    "                        print(\"🤖 Model思考中......\")\n",
    "                        process_query(full_query, qa_system, current_model)\n",
    "                    user_input = []\n",
    "                    break\n",
    "\n",
    "                user_input.append(line)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n输入中断，输入 exit 退出程序\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"系统错误：{str(e)}\")\n",
    "            print(\"❌ 发生意外错误，请重新尝试\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "81174f21210bc103"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
